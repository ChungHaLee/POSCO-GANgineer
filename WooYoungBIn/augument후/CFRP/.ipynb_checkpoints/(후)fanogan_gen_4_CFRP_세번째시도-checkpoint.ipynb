{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fAnoGAN_agu후\n",
    "\n",
    "- parameter   \n",
    "  : learning_rate = 3e-5  \n",
    "  : learning_G_per_D=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Graph & Animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 100  # z벡터의 잠재공간(latent space)의 크기\n",
    "workers = 4    # 0일때, 약 20% 사용 4일 경우 메모리 100%\n",
    "img_size = 64\n",
    "channel = 1\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 3e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset : 920\n",
      "device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 이미지 경로, 이미지들 리사이즈 및 텐서형태로 만들기\n",
    "data_root = \"/home/piai/Desktop/aug_train\"\n",
    "\n",
    "data_set = dataset.ImageFolder(root = data_root,\n",
    "                           transform = transforms.Compose([\n",
    "                                  #transforms.Resize(img_size),\n",
    "                                  transforms.CenterCrop(img_size),\n",
    "                                  torchvision.transforms.Grayscale(channel),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5,),(0.5,))\n",
    "                              ]))\n",
    "\n",
    "print(\"size of dataset :\", len(data_set))\n",
    "\n",
    "# 배치로 나누고 셔플하기\n",
    "data_loader = torch.utils.data.DataLoader(data_set, batch_size = batch_size,\n",
    "                                         shuffle = True, num_workers = workers, drop_last=True)\n",
    "\n",
    "# Device setting (GPU or CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device :\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast AnoGAN : Generative Adversarial Networks Model with AutoEncoder\n",
    "\n",
    "# === Generator 모델 ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        \n",
    "        # Batch Normalization 유무에 따른 G_block 정의\n",
    "        def G_block(in_features, out_features, FIRST=True):\n",
    "            if FIRST:\n",
    "                block = [\n",
    "                    nn.ConvTranspose2d(in_features, out_features, 4, 1, 0, bias=False),\n",
    "                    nn.BatchNorm2d(out_features),\n",
    "                    nn.ReLU()\n",
    "                ]\n",
    "            else:\n",
    "                block = [\n",
    "                    nn.ConvTranspose2d(in_features, out_features, 4, 2, 1, bias=False),\n",
    "                    nn.BatchNorm2d(out_features),\n",
    "                    nn.ReLU()\n",
    "                ]\n",
    "            return block\n",
    "        \n",
    "        \n",
    "        # ======================= 픽셀 분포 생성 layer ======================= \n",
    "        self.G_gen_distribution = nn.Sequential(\n",
    "            # ------ input is latent_size 100 ------ \n",
    "            *G_block(latent_size, img_size*8, FIRST=True),\n",
    "            # ------ state size is 512x4x4 ------ \n",
    "            *G_block(img_size*8, img_size*4, FIRST=False),\n",
    "            # ------ state size is 256x8x8 ------ \n",
    "            *G_block(img_size*4, img_size*2, FIRST=False),\n",
    "            # ------ state size is 128x16x16 ------ \n",
    "            *G_block(img_size*2, img_size, FIRST=False),\n",
    "        )\n",
    "        \n",
    "        # =================== 가짜 이미지 생성 layer =================== \n",
    "        self.G_gen_fake_img = nn.Sequential(\n",
    "            # ------ state size is 64x32x32 ------ \n",
    "            nn.ConvTranspose2d(img_size, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() # 픽셀값의 범위 : -1 ~ 1로 두기 위해서\n",
    "            # ------ state size is 1x64x64 ------ \n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        distribution = self.G_gen_distribution(input)\n",
    "        fake_img = self.G_gen_fake_img(distribution)\n",
    "        \n",
    "        return fake_img\n",
    "\n",
    "\n",
    "# === Discriminator 모델 ===\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Batch Normalization 유무에 따른 D_block 정의\n",
    "        def D_block(in_features, out_features, BN=True):\n",
    "            if BN:\n",
    "                block = [\n",
    "                    nn.Conv2d(in_features, out_features, 4, 2, 1, bias=False),\n",
    "                    nn.BatchNorm2d(out_features),\n",
    "                    nn.LeakyReLU(0.2, inplace=True)\n",
    "                ]\n",
    "            else:\n",
    "                block = [\n",
    "                    nn.Conv2d(in_features, out_features, 4, 2, 1, bias=False),\n",
    "                    nn.LeakyReLU(0.2, inplace=True)\n",
    "                ]\n",
    "            return block\n",
    "        \n",
    "        \n",
    "        # ============== Feature 추출 layer ==============\n",
    "        self.D_extract_feature = nn.Sequential(\n",
    "            # ------ input is 1 x 64 x 64 ------ \n",
    "            *D_block(channel, img_size, BN=False),\n",
    "            # ------ state is 64 x 32 x 32 ------ \n",
    "            *D_block(img_size, img_size*2, BN=True),\n",
    "            # ------ state is 128 x 16 x 16 ------ \n",
    "            *D_block(img_size*2, img_size*4, BN=True),\n",
    "            # ------ state is 256 x 8 x 8 ------ \n",
    "            *D_block(img_size*4, img_size*8, BN=True)\n",
    "        )\n",
    "        \n",
    "        # ===================== 이진 분류 layer =====================\n",
    "        self.D_classification = nn.Sequential(        \n",
    "            # ------- state size 512x4x4 ------- \n",
    "            nn.Conv2d(img_size*8, channel, 4, 1, 0, bias=False),\n",
    "            #nn.Linear(fms*8*4*4, 1, bias=False),\n",
    "            nn.Sigmoid()        \n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        feature = self.D_extract_feature(input)\n",
    "        classification = self.D_classification(feature)\n",
    "        \n",
    "        return classification, feature\n",
    "\n",
    "\n",
    "# === Encoder Model ===\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        def E_block(in_features, out_features, BN=True):\n",
    "            if BN:\n",
    "                block = [\n",
    "                         nn.Conv2d(in_features, out_features, 4, 2, 1, bias=False),\n",
    "                         nn.BatchNorm2d(out_features),\n",
    "                         nn.LeakyReLU(0.2, inplace=True)\n",
    "                ]\n",
    "            else:\n",
    "                block = [\n",
    "                         nn.Conv2d(in_features, out_features, 4, 2, 3, bias=False),\n",
    "                         nn.LeakyReLU(0.2, inplace=True)\n",
    "                ]\n",
    "            return block\n",
    "        \n",
    "        # ============== Feature 추출 ==============\n",
    "        self.E_extract_feature = nn.Sequential(\n",
    "            # -------input is 1 x 64 x 64-------\n",
    "            *E_block(channel, img_size, BN=False),\n",
    "            # -------state is 64 x 32 x 32-------\n",
    "            *E_block(img_size, img_size*2, BN=True),\n",
    "            # -------state is 128 x 16 x 16-------\n",
    "            *E_block(img_size*2, img_size*4, BN=True),\n",
    "            # -------state is 256 x 8 x 8-------\n",
    "            *E_block(img_size*4, img_size*8, BN=True),\n",
    "        )\n",
    "        \n",
    "        # =============== Encoder Training layer ===============\n",
    "        self.E_validate = nn.Sequential(\n",
    "            # -------state is 512 x 4 x 4-------\n",
    "            nn.Conv2d(img_size*8, latent_size, 4, 1, 0, bias=False),\n",
    "            nn.Tanh()\n",
    "            # -------state is 100 x 97 x 97-------\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        feature = self.E_extract_feature(input)\n",
    "        validity = self.E_validate(feature)\n",
    "        \n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G and D 무게 초기화, classname 에 찾는 name가 없다면 -1 ,\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# D,G,E 네트워크 모델 객체 선언\n",
    "D = Discriminator().to(device)\n",
    "G = Generator().to(device)\n",
    "E = Encoder().to(device)\n",
    "\n",
    "# weight initialize/ nn.Module 클래스 안에 apply 함수가 정의되 있음, 각 함수들에 다 적용 하게한다\n",
    "D.apply(weights_init)\n",
    "G.apply(weights_init)\n",
    "E.apply(weights_init)\n",
    "\n",
    "# Binary cross entropy loss and optimizer\n",
    "DCGAN_criterion = nn.BCELoss()\n",
    "AE_criterion = nn.MSELoss()\n",
    "\n",
    "# latent vector에 배치 사이즈 64를 적용\n",
    "# 학습한 G로 새로운 것 만들어서 결과 확인 할때 사용\n",
    "noise_z = torch.randn(img_size, latent_size, 1, 1, device = device)\n",
    "\n",
    "# D와 G에 대해 두가지 최적화 설정\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr = learning_rate, betas=(0.5,0.999))\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr = learning_rate, betas=(0.5,0.999))\n",
    "E_optimizer = torch.optim.Adam(E.parameters(), lr = learning_rate, betas=(0.5,0.999))\n",
    "\n",
    "#print(D)\n",
    "#print(G)\n",
    "#print(E)\n",
    "\n",
    "def reset_grad():\n",
    "    D_optimizer.zero_grad()\n",
    "    G_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "def train_DCGAN(epoch, learning_G_per_D = 8):\n",
    "    global epochs\n",
    "    global iters\n",
    "    \n",
    "    # 인덱스 0부터 세기 시작\n",
    "    # data[0].size():64x1x64x64(image) / data[1].size():64(label)\n",
    "    for i,data in enumerate(data_loader,0):\n",
    "        \n",
    "        # Train D\n",
    "        real_img = data[0].to(device) # image size: 64x1x64x64(batch, channel, width, height)\n",
    "        b_size = real_img.size(0) # b_size = 64\n",
    "        real_labels = torch.ones(b_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(b_size, 1).to(device)\n",
    "        \n",
    "        # (--------------------------real-----------------------------)\n",
    "        real_classification, _ = D(real_img) # output = D(x)\n",
    "        real_loss = DCGAN_criterion(real_classification, real_labels) # D(x)=1일 때의 loss\n",
    "        real_score = real_classification\n",
    "        D_x = real_score.mean().item() \n",
    "            \n",
    "        # (--------------------------fake-----------------------------)\n",
    "        z = torch.randn(b_size, latent_size, 1, 1).to(device) # z size :64x100x1x1\n",
    "        fake_img = G(z)\n",
    "        fake_classification, _ = D(fake_img) # output = D(G(z))\n",
    "        fake_loss = DCGAN_criterion(fake_classification, fake_labels) # D(G(z))=0일 때의 loss\n",
    "        fake_score = fake_classification\n",
    "        D_G_z1 = fake_score.mean().item()\n",
    "\n",
    "        # (------------------Backprop and optimize---------------------)\n",
    "        D_loss = real_loss + fake_loss\n",
    "        reset_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step() # D(x)=1, D(G(z))=0이어야 D가 최적\n",
    "\n",
    "        \n",
    "        # Train G\n",
    "        #z = torch.randn(b_size,latent_size,1,1,device=device) # z size :64x100x1x1\n",
    "        for k in range(learning_G_per_D):\n",
    "            fake_img = G(z)\n",
    "            fake_classification,_ = D(fake_img)  # output : D(G(z))\n",
    "            D_G_z2 = fake_classification.mean().item()\n",
    "            G_loss = DCGAN_criterion(fake_classification, real_labels) # D(G(z))=1일 때의 loss=log(D(G(z)))\n",
    "\n",
    "            # (------------------Backprop and optimize---------------------)\n",
    "            reset_grad()\n",
    "            G_loss.backward()\n",
    "            G_optimizer.step() # D(G(z))=1 이어야 G가 최적\n",
    "            # ==> D(G(z))의 값이 0.5로 수렴해야 한다.\n",
    "        \n",
    "        \n",
    "        # print\n",
    "        print('[%d/%d][%d/%d]\\n- D_loss : %.4f / G_loss : %.4f\\n- D(x):%.4f / D(G(z1)) : %.4f / D(G(z2)) : %.4f' \n",
    "                   %(epoch+1, epochs, i, len(data_loader),D_loss.item(),\n",
    "                     G_loss.item(),D_x,D_G_z1,D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(G_loss.item())\n",
    "        D_losses.append(D_loss.item())\n",
    "        \n",
    "        #Check how the generator is doing by saving G's output on noise_z\n",
    "        if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(data_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = G(noise_z).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1\n",
    "        \n",
    "#torch.save(E.state_dict(), 'E.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100][0/14]\n",
      "- D_loss : 1.3859 / G_loss : 0.6744\n",
      "- D(x):0.5009 / D(G(z1)) : 0.5007 / D(G(z2)) : 0.5095\n",
      "[1/100][1/14]\n",
      "- D_loss : 1.3860 / G_loss : 0.6676\n",
      "- D(x):0.5038 / D(G(z1)) : 0.5036 / D(G(z2)) : 0.5130\n",
      "[1/100][2/14]\n",
      "- D_loss : 1.3913 / G_loss : 0.6598\n",
      "- D(x):0.5053 / D(G(z1)) : 0.5077 / D(G(z2)) : 0.5170\n",
      "[1/100][3/14]\n",
      "- D_loss : 1.3975 / G_loss : 0.6517\n",
      "- D(x):0.5065 / D(G(z1)) : 0.5119 / D(G(z2)) : 0.5212\n",
      "[1/100][4/14]\n",
      "- D_loss : 1.4008 / G_loss : 0.6462\n",
      "- D(x):0.5083 / D(G(z1)) : 0.5152 / D(G(z2)) : 0.5240\n",
      "[1/100][5/14]\n",
      "- D_loss : 1.4067 / G_loss : 0.6399\n",
      "- D(x):0.5094 / D(G(z1)) : 0.5191 / D(G(z2)) : 0.5273\n",
      "[1/100][6/14]\n",
      "- D_loss : 1.4112 / G_loss : 0.6336\n",
      "- D(x):0.5112 / D(G(z1)) : 0.5230 / D(G(z2)) : 0.5307\n",
      "[1/100][7/14]\n",
      "- D_loss : 1.4138 / G_loss : 0.6303\n",
      "- D(x):0.5130 / D(G(z1)) : 0.5259 / D(G(z2)) : 0.5325\n",
      "[1/100][8/14]\n",
      "- D_loss : 1.4114 / G_loss : 0.6267\n",
      "- D(x):0.5171 / D(G(z1)) : 0.5284 / D(G(z2)) : 0.5344\n",
      "[1/100][9/14]\n",
      "- D_loss : 1.4219 / G_loss : 0.6236\n",
      "- D(x):0.5144 / D(G(z1)) : 0.5309 / D(G(z2)) : 0.5360\n",
      "[1/100][10/14]\n",
      "- D_loss : 1.4193 / G_loss : 0.6229\n",
      "- D(x):0.5166 / D(G(z1)) : 0.5316 / D(G(z2)) : 0.5364\n",
      "[1/100][11/14]\n",
      "- D_loss : 1.4177 / G_loss : 0.6218\n",
      "- D(x):0.5189 / D(G(z1)) : 0.5330 / D(G(z2)) : 0.5370\n",
      "[1/100][12/14]\n",
      "- D_loss : 1.4191 / G_loss : 0.6205\n",
      "- D(x):0.5199 / D(G(z1)) : 0.5346 / D(G(z2)) : 0.5377\n",
      "[1/100][13/14]\n",
      "- D_loss : 1.4117 / G_loss : 0.6227\n",
      "- D(x):0.5229 / D(G(z1)) : 0.5338 / D(G(z2)) : 0.5365\n",
      "[2/100][0/14]\n",
      "- D_loss : 1.4130 / G_loss : 0.6225\n",
      "- D(x):0.5227 / D(G(z1)) : 0.5342 / D(G(z2)) : 0.5366\n",
      "[2/100][1/14]\n",
      "- D_loss : 1.4119 / G_loss : 0.6218\n",
      "- D(x):0.5238 / D(G(z1)) : 0.5346 / D(G(z2)) : 0.5370\n",
      "[2/100][2/14]\n",
      "- D_loss : 1.4115 / G_loss : 0.6235\n",
      "- D(x):0.5229 / D(G(z1)) : 0.5337 / D(G(z2)) : 0.5361\n",
      "[2/100][3/14]\n",
      "- D_loss : 1.4118 / G_loss : 0.6210\n",
      "- D(x):0.5240 / D(G(z1)) : 0.5348 / D(G(z2)) : 0.5374\n",
      "[2/100][4/14]\n",
      "- D_loss : 1.4080 / G_loss : 0.6220\n",
      "- D(x):0.5256 / D(G(z1)) : 0.5344 / D(G(z2)) : 0.5369\n",
      "[2/100][5/14]\n",
      "- D_loss : 1.4099 / G_loss : 0.6196\n",
      "- D(x):0.5265 / D(G(z1)) : 0.5362 / D(G(z2)) : 0.5381\n",
      "[2/100][6/14]\n",
      "- D_loss : 1.4085 / G_loss : 0.6194\n",
      "- D(x):0.5274 / D(G(z1)) : 0.5362 / D(G(z2)) : 0.5383\n",
      "[2/100][7/14]\n",
      "- D_loss : 1.4081 / G_loss : 0.6171\n",
      "- D(x):0.5288 / D(G(z1)) : 0.5373 / D(G(z2)) : 0.5395\n",
      "[2/100][8/14]\n",
      "- D_loss : 1.4147 / G_loss : 0.6163\n",
      "- D(x):0.5264 / D(G(z1)) : 0.5383 / D(G(z2)) : 0.5400\n",
      "[2/100][9/14]\n",
      "- D_loss : 1.4071 / G_loss : 0.6158\n",
      "- D(x):0.5313 / D(G(z1)) : 0.5390 / D(G(z2)) : 0.5402\n",
      "[2/100][10/14]\n",
      "- D_loss : 1.4111 / G_loss : 0.6143\n",
      "- D(x):0.5305 / D(G(z1)) : 0.5401 / D(G(z2)) : 0.5410\n",
      "[2/100][11/14]\n",
      "- D_loss : 1.4111 / G_loss : 0.6134\n",
      "- D(x):0.5313 / D(G(z1)) : 0.5407 / D(G(z2)) : 0.5415\n",
      "[2/100][12/14]\n",
      "- D_loss : 1.4086 / G_loss : 0.6144\n",
      "- D(x):0.5328 / D(G(z1)) : 0.5409 / D(G(z2)) : 0.5410\n",
      "[2/100][13/14]\n",
      "- D_loss : 1.4065 / G_loss : 0.6139\n",
      "- D(x):0.5348 / D(G(z1)) : 0.5416 / D(G(z2)) : 0.5413\n",
      "[3/100][0/14]\n",
      "- D_loss : 1.4086 / G_loss : 0.6143\n",
      "- D(x):0.5331 / D(G(z1)) : 0.5412 / D(G(z2)) : 0.5410\n",
      "[3/100][1/14]\n",
      "- D_loss : 1.4062 / G_loss : 0.6137\n",
      "- D(x):0.5347 / D(G(z1)) : 0.5415 / D(G(z2)) : 0.5413\n",
      "[3/100][2/14]\n",
      "- D_loss : 1.4080 / G_loss : 0.6129\n",
      "- D(x):0.5343 / D(G(z1)) : 0.5419 / D(G(z2)) : 0.5418\n",
      "[3/100][3/14]\n",
      "- D_loss : 1.4060 / G_loss : 0.6119\n",
      "- D(x):0.5366 / D(G(z1)) : 0.5429 / D(G(z2)) : 0.5423\n",
      "[3/100][4/14]\n",
      "- D_loss : 1.3984 / G_loss : 0.6134\n",
      "- D(x):0.5405 / D(G(z1)) : 0.5428 / D(G(z2)) : 0.5415\n",
      "[3/100][5/14]\n",
      "- D_loss : 1.4075 / G_loss : 0.6144\n",
      "- D(x):0.5353 / D(G(z1)) : 0.5425 / D(G(z2)) : 0.5410\n",
      "[3/100][6/14]\n",
      "- D_loss : 1.3947 / G_loss : 0.6159\n",
      "- D(x):0.5409 / D(G(z1)) : 0.5415 / D(G(z2)) : 0.5402\n",
      "[3/100][7/14]\n",
      "- D_loss : 1.4025 / G_loss : 0.6161\n",
      "- D(x):0.5370 / D(G(z1)) : 0.5416 / D(G(z2)) : 0.5401\n",
      "[3/100][8/14]\n",
      "- D_loss : 1.4018 / G_loss : 0.6182\n",
      "- D(x):0.5362 / D(G(z1)) : 0.5407 / D(G(z2)) : 0.5389\n",
      "[3/100][9/14]\n",
      "- D_loss : 1.3960 / G_loss : 0.6203\n",
      "- D(x):0.5387 / D(G(z1)) : 0.5402 / D(G(z2)) : 0.5378\n",
      "[3/100][10/14]\n",
      "- D_loss : 1.3963 / G_loss : 0.6221\n",
      "- D(x):0.5370 / D(G(z1)) : 0.5389 / D(G(z2)) : 0.5368\n",
      "[3/100][11/14]\n",
      "- D_loss : 1.3958 / G_loss : 0.6195\n",
      "- D(x):0.5370 / D(G(z1)) : 0.5385 / D(G(z2)) : 0.5382\n",
      "[3/100][12/14]\n",
      "- D_loss : 1.3927 / G_loss : 0.6155\n",
      "- D(x):0.5402 / D(G(z1)) : 0.5399 / D(G(z2)) : 0.5404\n",
      "[3/100][13/14]\n",
      "- D_loss : 1.3984 / G_loss : 0.6127\n",
      "- D(x):0.5395 / D(G(z1)) : 0.5419 / D(G(z2)) : 0.5419\n",
      "[4/100][0/14]\n",
      "- D_loss : 1.4000 / G_loss : 0.6111\n",
      "- D(x):0.5409 / D(G(z1)) : 0.5439 / D(G(z2)) : 0.5428\n",
      "[4/100][1/14]\n",
      "- D_loss : 1.3977 / G_loss : 0.6102\n",
      "- D(x):0.5429 / D(G(z1)) : 0.5444 / D(G(z2)) : 0.5432\n",
      "[4/100][2/14]\n",
      "- D_loss : 1.3981 / G_loss : 0.6098\n",
      "- D(x):0.5437 / D(G(z1)) : 0.5453 / D(G(z2)) : 0.5435\n",
      "[4/100][3/14]\n",
      "- D_loss : 1.3996 / G_loss : 0.6112\n",
      "- D(x):0.5428 / D(G(z1)) : 0.5453 / D(G(z2)) : 0.5427\n",
      "[4/100][4/14]\n",
      "- D_loss : 1.4008 / G_loss : 0.6141\n",
      "- D(x):0.5413 / D(G(z1)) : 0.5445 / D(G(z2)) : 0.5412\n",
      "[4/100][5/14]\n",
      "- D_loss : 1.3929 / G_loss : 0.6183\n",
      "- D(x):0.5435 / D(G(z1)) : 0.5428 / D(G(z2)) : 0.5389\n",
      "[4/100][6/14]\n",
      "- D_loss : 1.3939 / G_loss : 0.6227\n",
      "- D(x):0.5402 / D(G(z1)) : 0.5405 / D(G(z2)) : 0.5365\n",
      "[4/100][7/14]\n",
      "- D_loss : 1.3872 / G_loss : 0.6254\n",
      "- D(x):0.5412 / D(G(z1)) : 0.5381 / D(G(z2)) : 0.5351\n",
      "[4/100][8/14]\n",
      "- D_loss : 1.3897 / G_loss : 0.6233\n",
      "- D(x):0.5390 / D(G(z1)) : 0.5375 / D(G(z2)) : 0.5362\n",
      "[4/100][9/14]\n",
      "- D_loss : 1.3873 / G_loss : 0.6188\n",
      "- D(x):0.5422 / D(G(z1)) : 0.5390 / D(G(z2)) : 0.5386\n",
      "[4/100][10/14]\n",
      "- D_loss : 1.3923 / G_loss : 0.6175\n",
      "- D(x):0.5418 / D(G(z1)) : 0.5410 / D(G(z2)) : 0.5393\n",
      "[4/100][11/14]\n",
      "- D_loss : 1.3933 / G_loss : 0.6194\n",
      "- D(x):0.5416 / D(G(z1)) : 0.5412 / D(G(z2)) : 0.5383\n",
      "[4/100][12/14]\n",
      "- D_loss : 1.3955 / G_loss : 0.6200\n",
      "- D(x):0.5391 / D(G(z1)) : 0.5402 / D(G(z2)) : 0.5380\n",
      "[4/100][13/14]\n",
      "- D_loss : 1.3861 / G_loss : 0.6204\n",
      "- D(x):0.5442 / D(G(z1)) : 0.5402 / D(G(z2)) : 0.5377\n",
      "[5/100][0/14]\n",
      "- D_loss : 1.3881 / G_loss : 0.6236\n",
      "- D(x):0.5426 / D(G(z1)) : 0.5398 / D(G(z2)) : 0.5360\n",
      "[5/100][1/14]\n",
      "- D_loss : 1.3862 / G_loss : 0.6286\n",
      "- D(x):0.5417 / D(G(z1)) : 0.5381 / D(G(z2)) : 0.5333\n",
      "[5/100][2/14]\n",
      "- D_loss : 1.3753 / G_loss : 0.6350\n",
      "- D(x):0.5441 / D(G(z1)) : 0.5351 / D(G(z2)) : 0.5300\n",
      "[5/100][3/14]\n",
      "- D_loss : 1.3737 / G_loss : 0.6396\n",
      "- D(x):0.5408 / D(G(z1)) : 0.5316 / D(G(z2)) : 0.5275\n",
      "[5/100][4/14]\n",
      "- D_loss : 1.3633 / G_loss : 0.6438\n",
      "- D(x):0.5441 / D(G(z1)) : 0.5296 / D(G(z2)) : 0.5253\n",
      "[5/100][5/14]\n",
      "- D_loss : 1.3630 / G_loss : 0.6494\n",
      "- D(x):0.5417 / D(G(z1)) : 0.5270 / D(G(z2)) : 0.5224\n",
      "[5/100][6/14]\n",
      "- D_loss : 1.3543 / G_loss : 0.6524\n",
      "- D(x):0.5432 / D(G(z1)) : 0.5245 / D(G(z2)) : 0.5208\n",
      "[5/100][7/14]\n",
      "- D_loss : 1.3536 / G_loss : 0.6529\n",
      "- D(x):0.5427 / D(G(z1)) : 0.5236 / D(G(z2)) : 0.5205\n",
      "[5/100][8/14]\n",
      "- D_loss : 1.3491 / G_loss : 0.6515\n",
      "- D(x):0.5446 / D(G(z1)) : 0.5232 / D(G(z2)) : 0.5213\n",
      "[5/100][9/14]\n",
      "- D_loss : 1.3516 / G_loss : 0.6498\n",
      "- D(x):0.5443 / D(G(z1)) : 0.5242 / D(G(z2)) : 0.5221\n",
      "[5/100][10/14]\n",
      "- D_loss : 1.3627 / G_loss : 0.6492\n",
      "- D(x):0.5395 / D(G(z1)) : 0.5252 / D(G(z2)) : 0.5225\n",
      "[5/100][11/14]\n",
      "- D_loss : 1.3641 / G_loss : 0.6511\n",
      "- D(x):0.5383 / D(G(z1)) : 0.5248 / D(G(z2)) : 0.5215\n",
      "[5/100][12/14]\n",
      "- D_loss : 1.3549 / G_loss : 0.6535\n",
      "- D(x):0.5423 / D(G(z1)) : 0.5239 / D(G(z2)) : 0.5202\n",
      "[5/100][13/14]\n",
      "- D_loss : 1.3605 / G_loss : 0.6551\n",
      "- D(x):0.5381 / D(G(z1)) : 0.5229 / D(G(z2)) : 0.5194\n",
      "[6/100][0/14]\n",
      "- D_loss : 1.3543 / G_loss : 0.6581\n",
      "- D(x):0.5399 / D(G(z1)) : 0.5215 / D(G(z2)) : 0.5178\n",
      "[6/100][1/14]\n",
      "- D_loss : 1.3501 / G_loss : 0.6599\n",
      "- D(x):0.5410 / D(G(z1)) : 0.5206 / D(G(z2)) : 0.5169\n",
      "[6/100][2/14]\n",
      "- D_loss : 1.3552 / G_loss : 0.6627\n",
      "- D(x):0.5366 / D(G(z1)) : 0.5191 / D(G(z2)) : 0.5154\n",
      "[6/100][3/14]\n",
      "- D_loss : 1.3308 / G_loss : 0.6649\n",
      "- D(x):0.5490 / D(G(z1)) : 0.5181 / D(G(z2)) : 0.5143\n",
      "[6/100][4/14]\n",
      "- D_loss : 1.3376 / G_loss : 0.6684\n",
      "- D(x):0.5435 / D(G(z1)) : 0.5166 / D(G(z2)) : 0.5125\n",
      "[6/100][5/14]\n",
      "- D_loss : 1.3322 / G_loss : 0.6721\n",
      "- D(x):0.5445 / D(G(z1)) : 0.5148 / D(G(z2)) : 0.5107\n",
      "[6/100][6/14]\n",
      "- D_loss : 1.3320 / G_loss : 0.6740\n",
      "- D(x):0.5428 / D(G(z1)) : 0.5134 / D(G(z2)) : 0.5097\n",
      "[6/100][7/14]\n",
      "- D_loss : 1.3304 / G_loss : 0.6761\n",
      "- D(x):0.5427 / D(G(z1)) : 0.5122 / D(G(z2)) : 0.5086\n",
      "[6/100][8/14]\n",
      "- D_loss : 1.3152 / G_loss : 0.6792\n",
      "- D(x):0.5496 / D(G(z1)) : 0.5111 / D(G(z2)) : 0.5070\n",
      "[6/100][9/14]\n",
      "- D_loss : 1.3241 / G_loss : 0.6820\n",
      "- D(x):0.5430 / D(G(z1)) : 0.5096 / D(G(z2)) : 0.5056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/100][10/14]\n",
      "- D_loss : 1.3190 / G_loss : 0.6840\n",
      "- D(x):0.5444 / D(G(z1)) : 0.5083 / D(G(z2)) : 0.5046\n",
      "[6/100][11/14]\n",
      "- D_loss : 1.3296 / G_loss : 0.6853\n",
      "- D(x):0.5378 / D(G(z1)) : 0.5074 / D(G(z2)) : 0.5039\n",
      "[6/100][12/14]\n",
      "- D_loss : 1.3105 / G_loss : 0.6880\n",
      "- D(x):0.5473 / D(G(z1)) : 0.5067 / D(G(z2)) : 0.5026\n",
      "[6/100][13/14]\n",
      "- D_loss : 1.3098 / G_loss : 0.6912\n",
      "- D(x):0.5462 / D(G(z1)) : 0.5054 / D(G(z2)) : 0.5010\n",
      "[7/100][0/14]\n",
      "- D_loss : 1.3132 / G_loss : 0.6928\n",
      "- D(x):0.5426 / D(G(z1)) : 0.5038 / D(G(z2)) : 0.5002\n",
      "[7/100][1/14]\n",
      "- D_loss : 1.3026 / G_loss : 0.6920\n",
      "- D(x):0.5476 / D(G(z1)) : 0.5032 / D(G(z2)) : 0.5006\n",
      "[7/100][2/14]\n",
      "- D_loss : 1.3122 / G_loss : 0.6906\n",
      "- D(x):0.5431 / D(G(z1)) : 0.5036 / D(G(z2)) : 0.5013\n",
      "[7/100][3/14]\n",
      "- D_loss : 1.3043 / G_loss : 0.6890\n",
      "- D(x):0.5481 / D(G(z1)) : 0.5043 / D(G(z2)) : 0.5021\n",
      "[7/100][4/14]\n",
      "- D_loss : 1.3023 / G_loss : 0.6886\n",
      "- D(x):0.5507 / D(G(z1)) : 0.5055 / D(G(z2)) : 0.5023\n",
      "[7/100][5/14]\n",
      "- D_loss : 1.3114 / G_loss : 0.6923\n",
      "- D(x):0.5449 / D(G(z1)) : 0.5048 / D(G(z2)) : 0.5004\n",
      "[7/100][6/14]\n",
      "- D_loss : 1.3026 / G_loss : 0.6931\n",
      "- D(x):0.5483 / D(G(z1)) : 0.5035 / D(G(z2)) : 0.5000\n",
      "[7/100][7/14]\n",
      "- D_loss : 1.3043 / G_loss : 0.6948\n",
      "- D(x):0.5470 / D(G(z1)) : 0.5032 / D(G(z2)) : 0.4992\n",
      "[7/100][8/14]\n",
      "- D_loss : 1.3069 / G_loss : 0.6999\n",
      "- D(x):0.5443 / D(G(z1)) : 0.5020 / D(G(z2)) : 0.4966\n",
      "[7/100][9/14]\n",
      "- D_loss : 1.2892 / G_loss : 0.7078\n",
      "- D(x):0.5510 / D(G(z1)) : 0.4994 / D(G(z2)) : 0.4927\n",
      "[7/100][10/14]\n",
      "- D_loss : 1.3008 / G_loss : 0.7140\n",
      "- D(x):0.5406 / D(G(z1)) : 0.4955 / D(G(z2)) : 0.4897\n",
      "[7/100][11/14]\n",
      "- D_loss : 1.2882 / G_loss : 0.7182\n",
      "- D(x):0.5445 / D(G(z1)) : 0.4928 / D(G(z2)) : 0.4876\n",
      "[7/100][12/14]\n",
      "- D_loss : 1.2986 / G_loss : 0.7230\n",
      "- D(x):0.5362 / D(G(z1)) : 0.4906 / D(G(z2)) : 0.4853\n",
      "[7/100][13/14]\n",
      "- D_loss : 1.2782 / G_loss : 0.7255\n",
      "- D(x):0.5454 / D(G(z1)) : 0.4887 / D(G(z2)) : 0.4841\n",
      "[8/100][0/14]\n",
      "- D_loss : 1.2750 / G_loss : 0.7260\n",
      "- D(x):0.5461 / D(G(z1)) : 0.4877 / D(G(z2)) : 0.4838\n",
      "[8/100][1/14]\n",
      "- D_loss : 1.2824 / G_loss : 0.7259\n",
      "- D(x):0.5419 / D(G(z1)) : 0.4876 / D(G(z2)) : 0.4839\n",
      "[8/100][2/14]\n",
      "- D_loss : 1.2786 / G_loss : 0.7193\n",
      "- D(x):0.5448 / D(G(z1)) : 0.4883 / D(G(z2)) : 0.4871\n",
      "[8/100][3/14]\n",
      "- D_loss : 1.2854 / G_loss : 0.7186\n",
      "- D(x):0.5441 / D(G(z1)) : 0.4911 / D(G(z2)) : 0.4875\n",
      "[8/100][4/14]\n",
      "- D_loss : 1.2885 / G_loss : 0.7242\n",
      "- D(x):0.5425 / D(G(z1)) : 0.4910 / D(G(z2)) : 0.4847\n",
      "[8/100][5/14]\n",
      "- D_loss : 1.2581 / G_loss : 0.7349\n",
      "- D(x):0.5560 / D(G(z1)) : 0.4880 / D(G(z2)) : 0.4796\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_DCGAN(epoch, learning_G_per_D = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_gen_imgs():\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ims=[[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "    HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_gen_imgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trian : izif E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "D.eval()\n",
    "\n",
    "kappa = 1.0\n",
    "E_losses = []\n",
    "\n",
    "iters = 0\n",
    "AE_img_list = []\n",
    "\n",
    "def train_AE(epoch):\n",
    "    global epochs\n",
    "    global iters\n",
    "    \n",
    "    # 인덱스 0부터 세기 시작\n",
    "    # data[0].size():64x1x64x64(image) / data[1].size():64(label)\n",
    "    for i,data in enumerate(data_loader,0):\n",
    "        \n",
    "        real_imgs = data[0].to(device)\n",
    "        \n",
    "        E_optimizer.zero_grad()\n",
    "        \n",
    "        #print(real_imgs.shape)\n",
    "        E_validity = E(real_imgs)\n",
    "        \n",
    "        fake_imgs = G(E_validity)\n",
    "        \n",
    "        _, real_features = D.forward(real_imgs)\n",
    "        _, fake_features = D.forward(fake_imgs)\n",
    "        \n",
    "        \n",
    "        # izif architecture\n",
    "        imgs_loss = AE_criterion(real_imgs, fake_imgs)\n",
    "        features_loss = AE_criterion(real_features, fake_features)\n",
    "        E_loss = imgs_loss + kappa*features_loss\n",
    "        \n",
    "        E_loss.backward()\n",
    "        E_optimizer.step()\n",
    "        \n",
    "        # =============================================================\n",
    "        # print\n",
    "        # =============================================================\n",
    "        print('[%d/%d][%d/%d]\\n- E_loss: %.4f\\n'\n",
    "              %(epoch+1, epochs, i, len(data_loader), E_loss.item()))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        E_losses.append(E_loss.item())\n",
    "        \n",
    "        #Check how the generator is doing by saving G's output on noise_z\n",
    "        if (iters % 500 == 0) or ((epoch == epochs-1) and (i == len(data_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = G(E_validity).detach().cpu()\n",
    "            AE_img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1\n",
    "        \n",
    "#torch.save(E.state_dict(), 'E.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_AE(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_loss():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses, label=\"G\")\n",
    "    plt.plot(D_losses, label=\"D\")\n",
    "    plt.plot(E_losses, label=\"E\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_root = \"/home/piai/POSCO-GANgineer/OhSeHyeon/source/dataset/test/test_C\"\n",
    "test_data_set = dataset.ImageFolder(root = test_data_root,\n",
    "                           transform = transforms.Compose([\n",
    "                                  transforms.Resize(img_size),\n",
    "                                  transforms.CenterCrop(img_size),\n",
    "                                  torchvision.transforms.Grayscale(channel),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5,),(0.5,))\n",
    "                              ]))\n",
    "\n",
    "# 배치로 나누고 셔플하기\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data_set, batch_size = 1,\n",
    "                                              shuffle = False, num_workers = workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이상 픽셀 수 확인 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_el_not_0(diff_img):\n",
    "    count_el_not_0 = 0\n",
    "    \n",
    "    col_size = diff_img.shape[0]\n",
    "    row_size = diff_img.shape[1]\n",
    "    \n",
    "    #print(col_size, row_size)\n",
    "    \n",
    "    for col in range(col_size):\n",
    "        for row in range(row_size):\n",
    "            if diff_img[col][row] != 0:\n",
    "                count_el_not_0 += 1\n",
    "                \n",
    "    return count_el_not_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# img 비교 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imgs(real_img, generated_img, i, score, reverse=False, threshold=50):\n",
    "    real_img = real_img.cpu().data.numpy().reshape(img_size, img_size) * 255\n",
    "    generated_img = generated_img.cpu().data.numpy().reshape(img_size, img_size) * 255\n",
    "    \n",
    "    negative = np.zeros_like(real_img)\n",
    "\n",
    "    if not reverse:\n",
    "        diff_img = real_img - generated_img\n",
    "    else:\n",
    "        diff_img = generated_img - real_img\n",
    "\n",
    "    diff_img[diff_img <= threshold] = 0\n",
    "\n",
    "    # 분율 추출\n",
    "    diff_cnts.append(count_el_not_0(diff_img))\n",
    "    # 분산 추출\n",
    "    diff_points.append(np.where(diff_img > threshold))\n",
    "    \n",
    "    anomaly_img = np.zeros(shape=(img_size, img_size, 3))\n",
    "    anomaly_img[:, :, 0] = real_img - diff_img\n",
    "    anomaly_img[:, :, 1] = real_img - diff_img\n",
    "    anomaly_img[:, :, 2] = real_img - diff_img\n",
    "    anomaly_img[:, :, 0] = anomaly_img[:,:,0] + diff_img\n",
    "    anomaly_img = anomaly_img.astype(np.uint8)\n",
    "    \n",
    "    # anomaly_imgs\n",
    "    anomaly_imgs.append(anomaly_img)\n",
    "\n",
    "    fig, plots = plt.subplots(1, 4)\n",
    "\n",
    "    fig.suptitle(f'Anomaly - (anomaly score: {score:.4})')\n",
    "    \n",
    "    fig.set_figwidth(9)\n",
    "    fig.set_tight_layout(True)\n",
    "    plots = plots.reshape(-1)\n",
    "    plots[0].imshow(real_img, cmap='gray', label = \"real\")\n",
    "    plots[1].imshow(generated_img, cmap='gray')\n",
    "    plots[2].imshow(diff_img, cmap='gray')\n",
    "    plots[3].imshow(anomaly_img)\n",
    "    \n",
    "    plots[0].set_title('real')\n",
    "    plots[1].set_title('generated')\n",
    "    plots[2].set_title('difference')\n",
    "    plots[3].set_title('Anomaly Detection')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cnts = []\n",
    "diff_points = []\n",
    "anomaly_imgs = []\n",
    "\n",
    "ano_criterion = nn.MSELoss()\n",
    "G.eval()\n",
    "D.eval()\n",
    "E.eval()\n",
    "\n",
    "# with open(\"score.csv\", \"w\") as f:\n",
    "#         f.write(\"label,img_distance,anomaly_score,z_distance\\n\")\n",
    "\n",
    "for i, data in enumerate(test_data_loader, 0):\n",
    "    real_img = data[0].to(device)\n",
    "\n",
    "    real_z = E(real_img) # 진짜 이미지의 latent vector\n",
    "    fake_img = G(real_z) # G에 넣어서 가짜 이미지 생성.\n",
    "    fake_z = E(fake_img) # torch.Size([1, 100]) --> latent 진짜 이미지와 매핑된 가짜 이미지의 latent vector\n",
    "\n",
    "    _, real_feature = D.forward(real_img) # 1, 256\n",
    "    _, fake_feature = D.forward(fake_img)\n",
    "\n",
    "    img_distance = ano_criterion(fake_img, real_img)\n",
    "    loss_feature = ano_criterion(fake_feature, real_feature)\n",
    "\n",
    "    anomaly_score = img_distance + kappa*loss_feature\n",
    "\n",
    "    z_distance = ano_criterion(fake_z, real_z)\n",
    "    \n",
    "#     with open(\"score.csv\", \"a\") as f:\n",
    "#             f.write(f\"{label.item()},{img_distance},\"\n",
    "#                     f\"{anomaly_score},{z_distance}\\n\")\n",
    "            \n",
    "#     print(f\"{label.item()}, {img_distance}, \"\n",
    "#           f\"{anomaly_score}, {z_distance}\\n\")\n",
    "    compare_imgs(real_img, fake_img, i, anomaly_score, reverse = False, threshold = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분율 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cnts = np.array(diff_cnts)\n",
    "\n",
    "diff_fraction = diff_cnts / img_size ** 2\n",
    "\n",
    "print(diff_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분산 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from extended_int import int_inf\n",
    "\n",
    "corr_coeffis = []\n",
    "corr_p_vals = []\n",
    "\n",
    "def cal_corr_coeffis():\n",
    "    for idx in range(len(test_data_loader)):\n",
    "        x_points = diff_points[idx][0]\n",
    "        y_points = diff_points[idx][1]\n",
    "        \n",
    "        \n",
    "        if len(x_points) > 0:\n",
    "            corr_coeffi, corr_p_val = stats.pearsonr(x_points, y_points)\n",
    "        else:\n",
    "            corr_coeffi, corr_p_val = -int_inf, -int_inf\n",
    "        \n",
    "        corr_coeffis.append(corr_coeffi)\n",
    "        corr_p_vals.append(corr_p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_corr_coeffis()\n",
    "\n",
    "print(corr_coeffis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장 및 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = \"./pretrained/pretrained.pth\"\n",
    "\n",
    "def save_pretrained():\n",
    "    pretrained = {\n",
    "        \"D\" : D.state_dict(),\n",
    "        \"G\" : G.state_dict(),\n",
    "        \"E\" : E.state_dict()\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir(\"pretrained\"):\n",
    "        os.mkdir(\"pretrained\")\n",
    "    torch.save(pretrained, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_D = Discriminator().to(device)\n",
    "pretrained_G = Generator().to(device)\n",
    "pretrained_E = Encoder().to(device)\n",
    "\n",
    "def load_pretrained():\n",
    "    global pretrained_D\n",
    "    global pretrained_G\n",
    "    global pretrained_E\n",
    "    \n",
    "    assert os.path.isdir(\"pretrained\"), \"Error : no pretrained dir found!\"\n",
    "    \n",
    "    pretrained = torch.load(save_file)\n",
    "    \n",
    "    pretrained_D.load_state_dict(pretrained[\"D\"])\n",
    "    pretrained_G.load_state_dict(pretrained[\"G\"])\n",
    "    pretrained_E.load_state_dict(pretrained[\"E\"])\n",
    "    \n",
    "    #print(\"pretrained_D :\", pretrained_D)\n",
    "    #print(\"pretrained_G :\", pretrained_G)\n",
    "    #print(\"pretrained_E :\", pretrained_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    ano_criterion = nn.MSELoss()\n",
    "    pretrained_G.eval()\n",
    "    pretrained_D.eval()\n",
    "    pretrained_E.eval()\n",
    "\n",
    "    # with open(\"score.csv\", \"w\") as f:\n",
    "    #         f.write(\"label,img_distance,anomaly_score,z_distance\\n\")\n",
    "\n",
    "    for i, data in enumerate(test_data_loader, 0):\n",
    "        real_img = data[0].to(device)\n",
    "\n",
    "        real_z = pretrained_E(real_img) # 진짜 이미지의 latent vector\n",
    "        fake_img = pretrained_G(real_z) # G에 넣어서 가짜 이미지 생성.\n",
    "        fake_z = pretrained_E(fake_img) # torch.Size([1, 100]) --> latent 진짜 이미지와 매핑된 가짜 이미지의 latent vector\n",
    "\n",
    "        _, real_feature = pretrained_D.forward(real_img) # 1, 256\n",
    "        _, fake_feature = pretrained_D.forward(fake_img)\n",
    "\n",
    "        img_distance = ano_criterion(fake_img, real_img)\n",
    "        loss_feature = ano_criterion(fake_feature, real_feature)\n",
    "\n",
    "        anomaly_score = img_distance + kappa*loss_feature\n",
    "\n",
    "        z_distance = ano_criterion(fake_z, real_z)\n",
    "\n",
    "    #     with open(\"score.csv\", \"a\") as f:\n",
    "    #             f.write(f\"{label.item()},{img_distance},\"\n",
    "    #                     f\"{anomaly_score},{z_distance}\\n\")\n",
    "\n",
    "    #     print(f\"{label.item()}, {img_distance}, \"\n",
    "    #           f\"{anomaly_score}, {z_distance}\\n\")\n",
    "        compare_imgs(real_img, fake_img, i, anomaly_score, reverse = False, threshold = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cnts = []\n",
    "diff_points = []\n",
    "anomaly_imgs = []\n",
    "\n",
    "corr_coeffis = []\n",
    "corr_p_vals = []\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cnts = np.array(diff_cnts)\n",
    "\n",
    "diff_fraction = diff_cnts / img_size ** 2\n",
    "\n",
    "print(diff_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_corr_coeffis()\n",
    "\n",
    "print(corr_coeffis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
